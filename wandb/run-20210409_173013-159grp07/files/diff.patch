diff --git a/.gitignore b/.gitignore
index 789c53c..f1fbfec 100644
--- a/.gitignore
+++ b/.gitignore
@@ -162,4 +162,5 @@ data/*features.csv
 out/ts_logs
 out/*.npy
 
-secret_key.json
\ No newline at end of file
+secret_key.json
+*.csv
\ No newline at end of file
diff --git a/README.md b/README.md
index a1efc6e..e013633 100755
--- a/README.md
+++ b/README.md
@@ -4,25 +4,7 @@
 First paste the key in the JSON file. 
 
 ```shell
-$env:GOOGLE_APPLICATION_CREDENTIALS="credentials\secret_key.json"
-```
-## Setting up
 
-Set up the project directory, where the script and outpout should be.
-
-```shell
-$env:BUCKET_NAME="biomakers-autoencoder"
-
-$env:REGION="eu-west4"
-
-gsutil mb -l $REGION gs://$BUCKET_NAME
-```
-
-Package the Python module and upload to the bucket.
-
-```shell
-python setup.py sdist --formats=gztar
-```
 
 # AiPBAND deliverable 5.3 projects
 
diff --git a/requirements.txt b/requirements.txt
index bd54b92..a5bf110 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -2,4 +2,5 @@ tensorflow>2.2
 numpy
 scipy
 pandas
-sklearn
\ No newline at end of file
+sklearn
+python-decouple
\ No newline at end of file
diff --git a/setup.py b/setup.py
index 7cdcfa2..6b35e69 100644
--- a/setup.py
+++ b/setup.py
@@ -4,7 +4,8 @@ from setuptools import setup
 REQUIRED_PACKAGES = ['tensorflow>2.2',
                      'pandas',
                      'argparse',
-                     'sklearn']
+                     'sklearn',
+                     'python-decouple']
 
 setup(
     name='unsupervised',
@@ -19,4 +20,5 @@ setup(
     packages=find_packages(),
     include_package_data=True,
     description='SDAE'
+    moduleName='unsupervised/task'
 )
diff --git a/src/gutils.py b/src/gutils.py
deleted file mode 100644
index 492b93b..0000000
--- a/src/gutils.py
+++ /dev/null
@@ -1,39 +0,0 @@
-from google.cloud import storage
-
-
-def download_blob(bucket_name, source_blob_name, destination_file_name):
-    """Downloads a blob from the bucket."""
-    # bucket_name = "your-bucket-name"
-    # source_blob_name = "storage-object-name"
-    # destination_file_name = "local/path/to/file"
-
-    storage_client = storage.Client()
-
-    bucket = storage_client.bucket(bucket_name)
-
-    # Construct a client side representation of a blob.
-    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve
-    # any content from Google Cloud Storage. As we don't need additional data,
-    # using `Bucket.blob` is preferred here.
-    blob = bucket.blob(source_blob_name)
-    blob.download_to_filename(destination_file_name)
-
-    print(
-        "Blob {} downloaded to {}.".format(
-            source_blob_name, destination_file_name
-        )
-    )
-
-def get_blob(source_blob_name, destination_file_name):
-    # Construct a client side representation of a blob.
-    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve
-    # any content from Google Cloud Storage. As we don't need additional data,
-    # using `Bucket.blob` is preferred here.
-    blob = bucket.blob(source_blob_name)
-    blob.download_to_filename(destination_file_name)
-
-    print(
-        "Blob {} downloaded to {}.".format(
-            source_blob_name, destination_file_name
-        )
-    )
\ No newline at end of file
diff --git a/tasks.md b/tasks.md
deleted file mode 100644
index a08c586..0000000
--- a/tasks.md
+++ /dev/null
@@ -1,21 +0,0 @@
-# Configuration
-## Authentication
-
-First paste the key in the JSON file. 
-
-```shell
-$env:GOOGLE_APPLICATION_CREDENTIALS="credentials\secret_key.json"
-```
-## Setting up
-
-Set up the project directory, where the script and outpout should be.
-
-```shell
-$env:BUCKET_NAME="biomakers-autoencoder"
-
-$env:REGION="eu-west4"
-
-gsutil mb -l $REGION gs://$BUCKET_NAME
-```
-
-Package the Python module and upload to the bucket.
\ No newline at end of file
diff --git a/trainer/supervised/__init__.py b/trainer/supervised/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/trainer/supervised/task.py b/trainer/supervised/task.py
deleted file mode 100644
index d4d5660..0000000
--- a/trainer/supervised/task.py
+++ /dev/null
@@ -1,117 +0,0 @@
-# %%
-"""
-# supervised training of the encoder layers
-"""
-
-# %%
-"""
-## Initial setup
-### Imports
-"""
-
-# %%
-import tensorflow as tf
-print(tf.__version__)
-import pandas as pd
-import numpy as np
-from ../src/models import EncoderStack
-from tensorflow.keras.layers.experimental.preprocessing import Normalization
-from tensorflow.keras.utils import to_categorical
-from sklearn.model_selection import ShuffleSplit
-from sklearn.preprocessing import normalize
-import dataset
-import neptune
-import neptune_tensorboard as neptune_tb
-import os
-import logging
-
-# %%
-
-
-# %%
-"""
-### Set experiment configuration
-"""
-
-# %%
-config = {
-    "N_NODES": [1000, 500, 100],
-    "DROPOUT": [0.1],
-    "BATCH_SIZE": 15,
-    "EPOCHS": 5,
-    "TEST_RATIO": 0.30,
-    "DATA_BUCKET": "sdae-geo",
-    "DATA_OBJECT": "GEO_data_batch_corr_final.csv",
-    "DATA_LABELS": " GBM_class.csv",
-    "VERBOSITY": 2,
-    "LOG_DIR": "./log_dir",
-    "PATIENCE":3
-}
-
-
-# %%
-"""
-## Initialize Netptune and Tensorboard logging
-"""
-
-# %%
-os.environ['NEPTUNE_API_TOKEN']="eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiYjNiYmZhYjEtNzc3ZS00Y2NhLWI5NTgtYWU0MmQyMWJhM2I0In0="
-os.environ['NEPTUNE_PROJECT']="jgeof/sdae"
-os.environ['NEPTUNE_NOTEBOOK_ID']="ecd86e96-4da7-44e3-9a17-43da2dfcae35"
-os.environ['NEPTUNE_NOTEBOOK_PATH']="constrained-SDAE/sdae.ipynb"
-
-neptune.init(os.environ['NEPTUNE_PROJECT'], api_token=os.environ['NEPTUNE_API_TOKEN'])
-
-logger = logging.getLogger("SDAE")
-logger.setLevel(logging.DEBUG)
-logger.addHandler(logging.StreamHandler())
-
-experiment = neptune.create_experiment(name='configuration', params=config, logger=logger)
-
-os.environ['EXP_DIR'] = os.path.join(config["LOG_DIR"], experiment.id)
-os.mkdir(os.environ['EXP_DIR'])
-
-logger.info("project directory: {}".format(os.environ['EXP_DIR']))
-!neptune tensorboard ${EXP_DIR} --project ${NEPTUNE_PROJECT}
-%load_ext tensorboard
-
-# %%
-"""
-### Start tensorboard server
-Tensorboard by running the following command in a terminal:
-"""
-
-# %%
-print("tensorboard --logdir {} --bind_all".format(os.environ['EXP_DIR']))
-
-# %%
-"""
-**Tensorboard cannot server over HTTPS, use external HTTP url: http://34.77.45.86:6006/**
-"""
-
-# %%
-"""
-# Load and preprocess the data
-"""
-
-# %%
-"""
-## Load data from Google Storage
-"""
-
-# %%
-dataframe = dataset.load_gs_data(config['DATA_BUCKET'], config['DATA_OBJECT'], os.environ['EXP_DIR'])
-
-# %%
-classes = pd.read_csv('data/pd/class.csv', header=None, index_col=0).values
-classes = to_categorical(classes)
-
-
-model = EncoderStack(encoder_models, 'output/')
-
-print("\n##################################################################")
-print("Training layer {} with {} hidden nodes..\n".format(idx, num_hidden))
-loss_train, loss_test = model.fit(x_train, y_train, x_test, y_test, batch_size=BATCH_SIZE, num_epochs=EPOCHS)
-
-print("\nTraining losss: ", loss_train)
-print("\nTesting loss: ", loss_test)
\ No newline at end of file
diff --git a/trainer/unsupervised/__init__.py b/trainer/unsupervised/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/trainer/unsupervised/task.py b/trainer/unsupervised/task.py
deleted file mode 100644
index 2612f60..0000000
--- a/trainer/unsupervised/task.py
+++ /dev/null
@@ -1,135 +0,0 @@
-# Import comet_ml at the top of your file
-from comet_ml import Experiment
-
-
-
-
-import tensorflow as tf
-import argparse
-import pandas as pd
-import numpy as np
-from models import Autoencoder
-from sklearn.model_selection import ShuffleSplit
-from sklearn.preprocessing import normalize
-import os
-from config import config
-
-
-
-parser = argparse.ArgumentParser()
-
-parser.add_argument(
-    "--data-path",
-    type=str,
-    dest="data_path",
-    help="Path to the CSV input data. First row headers. First column IDs",
-    default="/data/GEO_features.csv",
-)
-
-parser.add_argument(
-    "--num-nodes",
-    default=[2000, 1000, 500],
-    dest="N_NODES",
-    metavar="N",
-    type=int,
-    nargs="+",
-    help="Number of nodes in each layer.",
-)
-parser.add_argument(
-    "--dropout",
-    default=[0.1],
-    dest="DROPOUT",
-    type=int,
-    nargs="+",
-    help="Number of nodes in each layer.",
-)
-parser.add_argument(
-    "--batch",
-    default=5,
-    dest="BATCH_SIZE",
-    type=int,
-    help="Number of samples per batch.",
-)
-parser.add_argument(
-    "--epochs", default=10, dest="EPOCHS", type=int, help="Number of epochs."
-)
-parser.add_argument(
-    "--test",
-    default=0.2,
-    dest="TEST_RATIO",
-    type=float,
-    help="Ratio of samples kept out for testing.",
-)
-parser.add_argument(
-    "--verbose",
-    default=1,
-    dest="VERBOSITY",
-    type=int,
-    choices=[0, 1, 2],
-    help="Verbosity level: 0 None, 1 Info, 2 All",
-)
-parser.add_argument(
-    "--tolerance",
-    default=3,
-    dest="PATIENCE",
-    type=int,
-    help="Tolenrance to the rate of improvment between each batch. Low values terminate quicker.",
-)
-args = parser.parse_args()
-
-# Report multiple hyperparameters using a dictionary:
-hyper_params = {
-    "num-nodes": args.N_NODES,
-    "dropout": args.DROPOUT,
-    "batch_size": args.BATCH_SIZE,
-    "epochs": args.EPOCHS,
-    "test_ratio": args.TEST_RATIO,
-    "tolerance": args.PATIENCE,
-}
-
-dataframe = pd.read_csv("./data/GEO_features.csv", index_col=0)
-data = dataframe.values
-data = normalize(data)
-
-rs = ShuffleSplit(n_splits=1, test_size=args.TEST_RATIO, random_state=0)
-split_itterator = rs.split(data)
-i_train, i_test = next(split_itterator)
-train_path = os.path.join("./out", "train_indices.npy")
-test_path = os.path.join("./out", "test_indices.npy")
-np.save(train_path, i_train)
-np.save(test_path, i_test)
-
-x_train, x_test = data[i_train], data[i_test]
-
-tensorboard_logs = "./out/ts_logs"
-
-
-x_train_out, x_test_out = x_train, x_test
-for idx, num_hidden in enumerate(args.N_NODES):
-    experiment = Experiment(**config)
-    experiment.log_parameters(hyper_params)
-
-    with experiment.train():
-        print("Training layer {} with {} hidden nodes..".format(idx, num_hidden))
-        encoder = Autoencoder(x_train_out.shape[1], num_hidden, tensorboard_logs)
-
-        recon_mse = encoder.fit(
-            x_train_out,
-            x_test_out,
-            batch_size=args.BATCH_SIZE,
-            num_epochs=args.EPOCHS,
-            verbose=args.VERBOSITY,
-            patience=args.PATIENCE,
-        )
-
-    with experiment.test():
-        x_train_out = encoder.encoder_model.predict(x_train_out)
-        x_test_out = encoder.encoder_model.predict(x_test_out)
-
-        print("Training losss for layer {}: {} ".format(idx, recon_mse[0]))
-        print("Testing loss for layer {}: {} ".format(idx, recon_mse[1]))
-
-        experiment.log_metrics({"trained_layer_mse": recon_mse[0], "test_layer_mse": recon_mse[1]})
-
-    model_path = os.path.join("encoders", "model_{}_{}".format(idx, num_hidden))
-    encoder.encoder_model.save(model_path)
